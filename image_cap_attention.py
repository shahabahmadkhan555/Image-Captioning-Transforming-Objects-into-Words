# -*- coding: utf-8 -*-
"""image_cap_attention.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sVmyor_kZDOwW-5EyEDmQBXgJwciJt18
"""

!pip install rouge

import string
from numpy import array
from pickle import load
from keras.preprocessing.text import Tokenizer
import matplotlib.pyplot as plt
#from keras.backend.tensorflow_backend import set_session
import keras
import collections
import tensorflow as tf
import sys, time, os, warnings 
warnings.filterwarnings("ignore")
import re
from rouge import Rouge 
import numpy as np
import pandas as pd 
from PIL import Image
import pickle
from collections import Counter
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from keras.utils import plot_model
from keras.models import Model
from keras.layers import Input
from keras.layers import Dense, BatchNormalization
from keras.layers import LSTM
from keras.layers import Embedding
from keras.layers import Dropout
from keras.layers.merge import add
from keras.callbacks import ModelCheckpoint
from keras.preprocessing.image import load_img, img_to_array
from sklearn.utils import shuffle
from keras.applications.vgg16 import VGG16, preprocess_input

from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle

from numpy import array
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
nltk.download('punkt')
nltk.download('stopwords')

processed_mapping={}
vocab=[]

def processing_tokens(text):
  tokens=word_tokenize(text)
  table=str.maketrans('','',string.punctuation)
  ptokens=[w.translate(table) for w in tokens]
  stri=[]
  for wd in ptokens:
    if wd.isalpha() and len(wd)>1:
      stri.append(wd)
  #stop_words = set(stopwords.words('english'))
  #words = [t for t in stri if not t in stop_words]
  normal_tokens=[w.lower() for w in stri]
 
  non_blank_tokens=[s for s in normal_tokens if s!='']

  processed_string=""
  for st in non_blank_tokens:
    processed_string+=" "+st
  processed_string=processed_string.strip()
  new_str=""
  new_str="<start> "+processed_string+" <end>"
  return new_str
def make_dictinary():
  f1=open("/content/drive/My Drive/ML_PROJECT_FOLDER/Flickr8k.token.txt","r")
  token_read=f1.read()
  tk=token_read.split("\n")
  #print(tk[-1])\
  tk=tk[:-1]
  dic_mapping={}
  for value in tk:
    map=value.split("\t")
    #print(map[1])
    tokens=processing_tokens(map[1])

    name,tag=map[0].split("#")
    try:
      dic_mapping[name].append(tokens)
    
    except:
      dic_mapping[name]=[]
      dic_mapping[name].append(tokens)
  return dic_mapping

def data_preprocessing():
  global processed_mapping;global vocab
  processed_mapping=make_dictinary()
  t=set()
  for key in processed_mapping.keys():
    for i in range(0,len(processed_mapping[key])):
      ll=processed_mapping[key][i].split(' ')
      for val in ll:
        if val!='' and val!=' ':
          t.add(val)
  tt=list(t)
  #print(tt)
  vocab.extend(tt)
  #print(processed_mapping)
  print(vocab[:5])
data_preprocessing()

def load_data(d,name):
    dbfile = open(d, 'wb') 
    pickle.dump(name, dbfile)
    dbfile.close()
def find_data(name):
    a= open(name, 'rb')
    p= pickle._Unpickler(a)
    p.encoding='latin1'
    db=p.load()
    return db
#load_data("processed_map",processed_mapping)
#global processed_mapping
#processed_mapping=find_data(processed_mapping)

features = find_data("/content/drive/My Drive/ML_PROJECT_FOLDER/features.p")

vocab.sort()

wd_ind={}
ind_wd={}
for i in range(0,len(vocab)):
  wd_ind[vocab[i]]=i
  ind_wd[i]=vocab[i]
train=[]
test=[]
f1=open("/content/drive/My Drive/ML_PROJECT_FOLDER/train_img.txt","r")
tr=f1.read()
train=tr.split("\n")
f1=open("/content/drive/My Drive/ML_PROJECT_FOLDER/test_img.txt","r")
te=f1.read()
test=te.split("\n")
print(vocab,len(wd_ind),len(ind_wd))

train=train[:200]
test=test[:100]

dir="/content/drive/My Drive/ML_PROJECT_FOLDER/Flicker8k_Dataset/"

train_captions = []
img_name_vector = []

for image_path in train:
  caption_list = processed_mapping[image_path]
  train_captions.extend(caption_list)
  imgp=dir+image_path
  img_name_vector.extend([image_path] * len(caption_list))

print(train_captions[0])
Image.open(dir+img_name_vector[0])

def calc_max_length(tensor):
    return max(len(t) for t in tensor)

top_k = 5000
tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,
                                                  oov_token="<unk>",
                                                  filters='!"#$%&()*+.,-/:;=?@[\]^_`{|}~ ')
tokenizer.fit_on_texts(train_captions)
train_seqs = tokenizer.texts_to_sequences(train_captions)

tokenizer.word_index['<pad>'] = 0
tokenizer.index_word[0] = '<pad>'
train_seqs = tokenizer.texts_to_sequences(train_captions)
cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')
max_length = calc_max_length(train_seqs)

img_to_cap_vector = collections.defaultdict(list)
for img, cap in zip(img_name_vector, cap_vector):
  img_to_cap_vector[img].append(cap)

# Create training and validation sets using an 80-20 split randomly.
img_keys = list(img_to_cap_vector.keys())
import random
random.shuffle(img_keys)

slice_index = int(len(img_keys)*0.8)
img_name_train_keys, img_name_val_keys = img_keys[:slice_index], img_keys[slice_index:]

img_name_train = []
cap_train = []
for imgt in img_name_train_keys:
  capt_len = len(img_to_cap_vector[imgt])
  img_name_train.extend([imgt] * capt_len)
  cap_train.extend(img_to_cap_vector[imgt])

img_name_val = []
cap_val = []
for imgv in img_name_val_keys:
  capv_len = len(img_to_cap_vector[imgv])
  img_name_val.extend([imgv] * capv_len)
  cap_val.extend(img_to_cap_vector[imgv])

len(img_name_train), len(cap_train), len(img_name_val), len(cap_val)

features[train[0]].shape

BATCH_SIZE = 1
BUFFER_SIZE = 1000
embedding_dim = 200
units = 512
vocab_size = top_k + 1
num_steps = len(img_name_train) // BATCH_SIZE
# Shape of the vector extracted from InceptionV3 is (64, 2048)
# These two variables represent that vector shape
features_shape = 2048
attention_features_shape = 1

def map_func(img_name, cap):
  print(img_name)
  return np.array(features[img_name]), cap
  #img_tensor = np.load(img_name.decode('utf-8')+'.npy')

dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))

# Use map to load the numpy files in parallel
dataset = dataset.map(lambda item1, item2: tf.numpy_function(
          map_func, [item1, item2], [tf.float32, tf.int32]),
          num_parallel_calls=tf.data.experimental.AUTOTUNE)

# Shuffle and batch
dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)
dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)

class BahdanauAttention(tf.keras.Model):
  def __init__(self, units):
    super(BahdanauAttention, self).__init__()
    self.W1 = tf.keras.layers.Dense(units)
    self.W2 = tf.keras.layers.Dense(units)
    self.V = tf.keras.layers.Dense(1)

  def call(self, features, hidden):
    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)

    # hidden shape == (batch_size, hidden_size)
    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)
    hidden_with_time_axis = tf.expand_dims(hidden, 1)

    # attention_hidden_layer shape == (batch_size, 64, units)
    attention_hidden_layer = (tf.nn.tanh(self.W1(features) +
                                         self.W2(hidden_with_time_axis)))

    # score shape == (batch_size, 64, 1)
    # This gives you an unnormalized score for each image feature.
    score = self.V(attention_hidden_layer)

    # attention_weights shape == (batch_size, 64, 1)
    attention_weights = tf.nn.softmax(score, axis=1)

    # context_vector shape after sum == (batch_size, hidden_size)
    context_vector = attention_weights * features
    context_vector = tf.reduce_sum(context_vector, axis=1)

    return context_vector, attention_weights

class CNN_Encoder(tf.keras.Model):
    # Since you have already extracted the features and dumped it using pickle
    # This encoder passes those features through a Fully connected layer
    def __init__(self, embedding_dim):
        super(CNN_Encoder, self).__init__()
        # shape after fc == (batch_size, 64, embedding_dim)
        self.fc = tf.keras.layers.Dense(embedding_dim)

    def call(self, x):
        x = self.fc(x)
        x = tf.nn.relu(x)
        return x

class RNN_Decoder(tf.keras.Model):
  def __init__(self, embedding_dim, units, vocab_size):
    super(RNN_Decoder, self).__init__()
    self.units = units

    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
    self.gru = tf.keras.layers.GRU(self.units,
                                   return_sequences=True,
                                   return_state=True,
                                   recurrent_initializer='glorot_uniform')
    self.fc1 = tf.keras.layers.Dense(self.units)
    self.fc2 = tf.keras.layers.Dense(vocab_size)

    self.attention = BahdanauAttention(self.units)

  def call(self, x, features, hidden):
    # defining attention as a separate model
    context_vector, attention_weights = self.attention(features, hidden)

    # x shape after passing through embedding == (batch_size, 1, embedding_dim)
    x = self.embedding(x)

    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)
    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)

    # passing the concatenated vector to the GRU
    output, state = self.gru(x)

    # shape == (batch_size, max_length, hidden_size)
    x = self.fc1(output)

    # x shape == (batch_size * max_length, hidden_size)
    x = tf.reshape(x, (-1, x.shape[2]))

    # output shape == (batch_size * max_length, vocab)
    x = self.fc2(x)

    return x, state, attention_weights

  def reset_state(self, batch_size):
    return tf.zeros((batch_size, self.units))

encoder = CNN_Encoder(embedding_dim)
decoder = RNN_Decoder(embedding_dim, units, vocab_size)

optimizer = tf.keras.optimizers.Adam()
loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits=True, reduction='none')

def loss_function(real, pred):
  mask = tf.math.logical_not(tf.math.equal(real, 0))
  loss_ = loss_object(real, pred)

  mask = tf.cast(mask, dtype=loss_.dtype)
  loss_ *= mask

  return tf.reduce_mean(loss_)

# checkpoint_path = "./checkpoints/train"
# ckpt = tf.train.Checkpoint(encoder=encoder,
#                            decoder=decoder,
#                            optimizer = optimizer)
# ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)
start_epoch = 0
# if ckpt_manager.latest_checkpoint:
#   start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])
#   # restoring the latest checkpoint in checkpoint_path
#   ckpt.restore(ckpt_manager.latest_checkpoint)
# adding this in a separate cell because if you run the training cell
# many times, the loss_plot array will be reset
loss_plot = []

tf.expand_dims([1,2,3,4],1)

@tf.function
def train_step(img_tensor, target):
  loss = 0
  #print("one")
  # initializing the hidden state for each batch
  # because the captions are not related from image to image
  hidden = decoder.reset_state(batch_size=target.shape[0])
  #print("two")

  temp=[0 for i in range(0,5001)]
  temp[tokenizer.word_index['<start>']]=1

  dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)
  #dec_input = tf.expand_dims(temp, 1)

  #print("three")
  #print(dec_input.shape)

  with tf.GradientTape() as tape:
      features = encoder(img_tensor)
      #print("four")
      #print(target.shape)

      for i in range(1, target.shape[0]):

          # passing the features through the decoder
          #print("five")
          predictions, hidden, _ = decoder(dec_input, features, hidden)
          #print("six")
          #print(predictions.shape)
          #print(hidden.shape)

          loss += loss_function(target[:,i], predictions)
          #print("seven")

          # using teacher forcing
          dec_input = tf.expand_dims(target[:,i], 1)
          #print("et")

  total_loss = (loss / int(target.shape[1]))

  trainable_variables = encoder.trainable_variables + decoder.trainable_variables

  gradients = tape.gradient(loss, trainable_variables)

  optimizer.apply_gradients(zip(gradients, trainable_variables))

  return loss, total_loss

print(len(img_name_train))
print(len(cap_train))

ll=[]
for i in range(0,len(img_name_train)):
  #temp=[0 for i in range(0,5001)]
  temp=np.zeros((25,5001))
  for j in range(len(cap_train[i])):
    temp[j][cap_train[i][j]]=1
  
  ll.append([features[img_name_train[i]],temp])

ll=np.array(ll)
ll[0][0].shape

ll[0][1].shape

for i in enumerate(ll):
  print(i);break

for (batch, (img_tensor, target)) in enumerate(ll):
  print(batch,img_tensor)
  break

EPOCHS = 10

for epoch in range(start_epoch, EPOCHS):
    start = time.time()
    total_loss = 0

    for (batch, (img_tensor, target)) in enumerate(ll):
        batch_loss, t_loss = train_step(img_tensor, target)
        total_loss += t_loss

        if batch % 100 == 0:
            print ('Epoch {} Batch {} Loss {:.4f}'.format(
              epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))
    # storing the epoch end loss value to plot later
    loss_plot.append(total_loss / num_steps)

    if epoch % 5 == 0:
      ckpt_manager.save()

    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,
                                         total_loss/num_steps))
    print ('Time taken for 1 epoch {} sec\n'.format(time.time() - start))

import matplotlib.pyplot as plt

plt.plot(loss_plot)
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Loss Plot')
plt.show()

import matplotlib.pyplot as plt

plt.plot(loss_plot)
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Loss Plot')
plt.show()

a=np.array([1,2,3,4])
a.reshape(2,2,-1).shape

def evaluate(image):
    attention_plot = np.zeros((max_length, attention_features_shape))

    hidden = decoder.reset_state(batch_size=1)

    '''temp_input = tf.expand_dims(load_image(image)[0], 0)
    img_tensor_val = image_features_extract_model(temp_input)
    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))'''


    features1 = encoder(features[image])
    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)
    result = []

    for i in range(max_length):
        predictions, hidden, attention_weights = decoder(dec_input, features1, hidden)

        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()

        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()
        result.append(tokenizer.index_word[predicted_id])

        if tokenizer.index_word[predicted_id] == '<end>':
            return result, attention_plot

        dec_input = tf.expand_dims([predicted_id], 0)

    attention_plot = attention_plot[:len(result), :]
    return result, attention_plot

def plot_attention(image, result, attention_plot):
    temp_image = np.array(Image.open(dir+image))

    fig = plt.figure(figsize=(10, 10))

    len_result = len(result)
    for l in range(len_result):
        temp_att = np.resize(attention_plot[l], (8, 8))
        ax = fig.add_subplot(len_result//2, len_result//2, l+1)
        ax.set_title(result[l])
        img = ax.imshow(temp_image)
        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())

    plt.tight_layout()
    plt.show()

rid = np.random.randint(0, len(img_name_val))
image = img_name_val[rid]
real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])
result, attention_plot = evaluate(image)

print ('Real Caption:', real_caption)
print ('Prediction Caption:', ' '.join(result))
plot_attention(image, result, attention_plot)



