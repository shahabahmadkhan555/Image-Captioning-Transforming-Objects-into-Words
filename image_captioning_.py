# -*- coding: utf-8 -*-
"""image_captioning_.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sRqReDscQsrkPQHVgFO5XWC_NHQvFN0k
"""

from google.colab import drive
drive.mount('/content/drive')

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Load pre-trained model (weights)
model = BertModel.from_pretrained('bert-base-uncased')
model.eval()

!pip install rouge

import numpy as np
import pandas as pd
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
import string
import pickle
from keras.applications.xception import Xception, preprocess_input
import os
import glob 
import matplotlib.pyplot as plt 

from PIL import Image
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from keras.layers.merge import add
from keras.models import Model, load_model
from keras.layers import Input, Dense, LSTM, Embedding, Dropout
from keras.utils import plot_model
import nltk.translate.bleu_score as bleu
import nltk.translate.gleu_score as gleu
#from rouge import Rouge

from numpy import array
import nltk
nltk.download('punkt')
nltk.download('stopwords')

processed_mapping={}
vocab=[]

def processing_tokens(text):
  tokens=word_tokenize(text)
  table=str.maketrans('','',string.punctuation)
  ptokens=[w.translate(table) for w in tokens]
  stri=[]
  for wd in ptokens:
    if wd.isalpha() and len(wd)>1:
      stri.append(wd)
  #stop_words = set(stopwords.words('english'))
  #words = [t for t in stri if not t in stop_words]
  normal_tokens=[w.lower() for w in stri]
 
  non_blank_tokens=[s for s in normal_tokens if s!='']

  processed_string=""
  for st in non_blank_tokens:
    processed_string+=" "+st
  processed_string=processed_string.strip()
  new_str=""
  new_str="<start> "+processed_string+" <end>"
  return new_str
def make_dictinary():
  f1=open("/content/drive/My Drive/ML_PROJECT_FOLDER/Flickr8k.token.txt","r")
  token_read=f1.read()
  tk=token_read.split("\n")
  #print(tk[-1])\
  tk=tk[:-1]
  dic_mapping={}
  for value in tk:
    map=value.split("\t")
    #print(map[1])
    tokens=processing_tokens(map[1])

    name,tag=map[0].split("#")
    try:
      dic_mapping[name].append(tokens)
    
    except:
      dic_mapping[name]=[]
      dic_mapping[name].append(tokens)
  return dic_mapping

def data_preprocessing():
  global processed_mapping;global vocab
  processed_mapping=make_dictinary()
  t=set()
  for key in processed_mapping.keys():
    for i in range(0,len(processed_mapping[key])):
      ll=processed_mapping[key][i].split(' ')
      for val in ll:
        if val!='' and val!=' ':
          t.add(val)
  tt=list(t)
  #print(tt)
  vocab.extend(tt)
  #print(processed_mapping)
  print(vocab[:5])
data_preprocessing()

def load_data(d,name):
    dbfile = open(d, 'wb') 
    pickle.dump(name, dbfile)
    dbfile.close()
def find_data(name):
    a= open(name, 'rb')
    p= pickle._Unpickler(a)
    p.encoding='latin1'
    db=p.load()
    return db
#load_data("processed_map",processed_mapping)
#global processed_mapping
#processed_mapping=find_data(processed_mapping)

features = find_data("/content/drive/My Drive/ML_PROJECT_FOLDER/features.p")

vocab.sort()

wd_ind={
    
}
ind_wd={}
for i in range(0,len(vocab)):
  wd_ind[vocab[i]]=i
  ind_wd[i]=vocab[i]

train=[]
test=[]
f1=open("/content/drive/My Drive/ML_PROJECT_FOLDER/train_img.txt","r")
tr=f1.read()
train=tr.split("\n")
f1=open("/content/drive/My Drive/ML_PROJECT_FOLDER/test_img.txt","r")
te=f1.read()
test=te.split("\n")
print(vocab,len(wd_ind),len(ind_wd))

processed_mapping["1240297429_c36ae0c58f.jpg"]

print(len(train))

max_len=0
def maximum_description():
  global processed_mapping; global max_len;tt=0
  for key in processed_mapping.keys():
    for cap in processed_mapping[key]:
      cp=cap.split(" ")
      #print(cp)
      max_len=max(max_len,len(cp))
  print(max_len,tt)
maximum_description()

from keras.preprocessing.text import Tokenizer
lis_data=[]
for key in processed_mapping.keys():
  for value in processed_mapping[key]:
    lis_data.append(value)
tokenizer = Tokenizer()
tokenizer.fit_on_texts(lis_data)

vocab_size = len(wd_ind) + 1 # one for appended 0's
vocab_size

def batch_generation(num_step):
  img_inp=[];text_inp=[];out_pred=[];
  global max_len;global processed_mapping;global features;
  lim=0
  while 1:
    for key in train:
      lim+=1
      ft=features[key][0]
      for value in processed_mapping[key]:
        embed=[wd_ind[word] for word in value.split(' ') if word in wd_ind]
        #embed=seq = [wd_ind[word] for word in value.split(' ') if word in wd_ind]

        for ind in range(0,len(embed)):
          a=embed[:ind]
          b=embed[ind]
          a=pad_sequences([a], maxlen=max_len)[0]
          b = to_categorical([b], num_classes=vocab_size)[0]
          img_inp.append(ft);text_inp.append(a);out_pred.append(b)
      if lim==num_step:
        yield ([array(img_inp), array(text_inp)], array(out_pred))
        img_inp=[];text_inp=[];out_pred=[];
        lim=0
#[s,t],u=next(batch_generation(3))
#print(s.shape,t.shape,u.shape)

embeddings_index = {} # empty dictionary
f = open("/content/drive/My Drive/ML_PROJECT_FOLDER/glove.6B.200d.txt","r")

for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()
print('Found %s word vectors.' % len(embeddings_index))

embedding_dim = 200

# Get 200-dim dense vector for each of the 10000 words in out vocabulary
embedding_matrix = np.zeros((vocab_size, embedding_dim))

for word, i in wd_ind.items():
    #if i < max_words:
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # Words not found in the embedding index will be all zeros
        embedding_matrix[i] = embedding_vector

embedding_matrix.shape

inputs1 = Input(shape=(2048,))
fe1 = Dropout(0.5)(inputs1)
fe2 = Dense(256, activation='relu')(fe1)
inputs2 = Input(shape=(max_len,))
se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)
se2 = Dropout(0.5)(se1)
se3 = LSTM(256)(se2)
decoder1 = add([fe2, se3])
decoder2 = Dense(256, activation='relu')(decoder1)
outputs = Dense(vocab_size, activation='softmax')(decoder2)
model = Model(inputs=[inputs1, inputs2], outputs=outputs)
model.layers[2].set_weights([embedding_matrix])
model.layers[2].trainable = False
model.compile(loss='categorical_crossentropy', optimizer='adam')    
# summarize model
print(model.summary())

epochs = 50
num_per_batch = 3
steps = len(train)//num_per_batch
from keras.callbacks import History 
H = History()
#model.layers[2].set_weights([embedding_matrix])
#model.layers[2].trainable = False
train_loss=[]
for i in range(epochs):
    generator = batch_generation(num_per_batch)
    H=model.fit_generator(generator,epochs=1, steps_per_epoch=steps, verbose=1)
    train_loss.append(H.history["loss"])
    #model.save('model_' + str(i) + '.h5')

#model.save('/content/drive/My Drive/model_embed_original')

import keras
model = keras.models.load_model('/content/drive/My Drive/model_embed_original/')

epochs = 90
num_per_batch = 3
steps = len(train)//num_per_batch
from keras.callbacks import History 
H = History()
#model.layers[2].set_weights([embedding_matrix])
#model.layers[2].trainable = False
train_loss=[]
for i in range(epochs):
    generator = batch_generation(num_per_batch)
    H=model.fit_generator(generator,epochs=1, steps_per_epoch=steps, verbose=1)
    train_loss.append(H.history["loss"])
    #model.save('model_' + str(i) + '.h5')

#model.save('/content/drive/My Drive/model_embed_90')

plt.plot(range(1,91),train_loss) 
plt.xlabel('Epoch') 
plt.ylabel('loss') 
plt.title('Training loss epoch curve')  
plt.show()

def bleu_score_cal(reference,hypothesis):
  ref=[];hyp=hypothesis.split(" ")
  for value in reference:
    t=value.split(" ")
    ref.append(t)
  #print("BLEU SCORE")
  score_1gram = bleu.sentence_bleu(ref, hyp, weights=(1,0,0,0))
  score_2gram = bleu.sentence_bleu(ref, hyp, weights=(0,1,0,0))
  score_3gram = bleu.sentence_bleu(ref, hyp, weights=(0,0,1,0))
  score_4gram = bleu.sentence_bleu(ref, hyp, weights=(0,0,0,1))
  #print("N-grams: 1: {}, 2: {}, 3: {}, 4: {}".format(score_1gram, score_2gram, score_3gram, score_4gram))
  score_ngram = bleu.sentence_bleu(ref, hyp, weights=(0.25,0.25,0.25,0.25))
  #print("N-grams: {} ".format( score_ngram))
  return score_1gram,score_2gram,score_3gram,score_4gram,score_ngram,


    

def gleu_score_cal(reference,hypothesis):
  ref=[];hyp=hypothesis.split(" ")
  for value in reference:
    t=value.split(" ")
    ref.append(t)
  score_ref_ab = gleu.sentence_gleu(ref, hyp)
  #print("Gleu Score: {}".format(score_ref_ab))
  return score_ref_ab


def roug_score_cal(reference,hypothesis):
  rouge = Rouge()
  f_s=0;p_s=0;r_s=0
  
  for value in reference:
    scores = rouge.get_scores(hypothesis,value)
    f_s+=float(scores[0]["rouge-l"]["f"])
    p_s+=float(scores[0]["rouge-l"]["p"])
    r_s+=float(scores[0]["rouge-l"]["r"])
  #print("Rouge Score: f: {}, P: {}, R: {}".format(f_s/5,p_s/5,r_s/5))
  return f_s/5,p_s/5,r_s/5

test=test[:-1]

import matplotlib.pyplot as plt
ll=0
a1=0;b1=0;c1=0;d1=0;e1=0;f1=0;g1=0;h1=0;i11=0
for tes_img in test:
  ll+=1
  generated_text="<start>"
  for i in range(0,max_len):
    emb=[wd_ind[word] for word in generated_text.split(' ') if word in wd_ind]
    #emb=tokenizer.texts_to_sequences([generated_text])[0]
    emb=pad_sequences([emb], maxlen=max_len)[0]
    i1=features[tes_img][0].reshape(1,-1)
    i2=emb.reshape(1,-1)
    next_word= model.predict_on_batch([i1,i2])
    pred_arg = np.argmax(next_word)
    value=None
    for a, b in wd_ind.items():
      if b == pred_arg:
        value=a
        break
    #print(ind_wd[pred_arg])
    try:
      value = ind_wd[pred_arg]
    except Exception as e:
      value=None
    if value!=None:
      generated_text+=" "+value
    if value=="<end>":
      break
  #print("For image")
  if ll<=5:
    x=plt.imread("/content/drive/My Drive/ML_PROJECT_FOLDER/Flicker8k_Dataset/"+tes_img)
    plt.imshow(x)
    plt.show()
    st=generated_text.split(" ")
    ans=""
    for i in range(1,len(st)-1):
      ans+=st[i]+" "
    lis_cap=processed_mapping[tes_img]
    hp="<start> "+ans.strip()+" <end>"
    #print(hp)
    #print(lis_cap)
    print(ans)
  t1,t2,t3,t4,t5=bleu_score_cal(lis_cap,hp)
  a1+=t1;b1+=t2;c1+=t3;d1+=t4;e1+=t5
  f1+=gleu_score_cal(lis_cap,hp)
  t1,t2,t3=roug_score_cal(lis_cap,hp)
  g1+=t1;h1+=t2;i11+=t3
  
print("N-grams: 1: {}, 2: {}, 3: {}, 4: {}, n: {} ".format(a1/len(test), b1/len(test),c1/len(test),d1/len(test),e1/len(test)))
print("Gleu Score: {}".format(f1/len(test)))
print("Rouge Score: f: {}, P: {}, R: {}".format(g1/len(test),h1/len(test),i11/len(test)))

def beam_search_pred(tes_img, K_beams = 3, log = False):
    start = [wd_ind["<start>"]]
    
    start_word = [[start, 0.0]]
    
    while len(start_word[0][0]) < max_len:
        temp = []
        for s in start_word:
            sequence  = pad_sequences([s[0]], maxlen=max_len).reshape((1,max_len)) #sequence of most probable words 
            i1=features[tes_img][0]                                                                    # based on the previous steps
            preds = model.predict([i1.reshape(1,2048), sequence])
            word_preds = np.argsort(preds[0])[-K_beams:] # sort predictions based on the probability, then take the last
                                                         # K_beams items. words with the most probs
            # Getting the top <K_beams>(n) predictions and creating a 
            # new list so as to put them via the model again
            for w in word_preds:
                
                next_cap, prob = s[0][:], s[1]
                next_cap.append(w)
                if log:
                    prob += np.log(preds[0][w]) # assign a probability to each K words4
                else:
                    prob += preds[0][w]
                temp.append([next_cap, prob])
        start_word = temp
        # Sorting according to the probabilities
        start_word = sorted(start_word, reverse=False, key=lambda l: l[1])

        # Getting the top words
        start_word = start_word[-K_beams:]
    
    start_word = start_word[-1][0]
    captions_ = [ind_wd[i] for i in start_word]
    #print(captions_)
    final_caption = []
    
    for i in captions_:
        if i != "<end>":
            final_caption.append(i)
        else:
            break
    
    final_caption = ' '.join(final_caption[1:])
    return final_caption
print("Beam 3")
for i in range(0,5):
  beam_3=beam_search_pred(test[i],  K_beams = 3, log = False)
  x=plt.imread("/content/drive/My Drive/ML_PROJECT_FOLDER/Flicker8k_Dataset/"+test[i])
  plt.imshow(x)
  plt.show()
  print(beam_3)
  st=beam_3.split(" ")
  ans=""
  for i in range(1,len(st)-1):
    ans+=st[i]+" "
  lis_cap=processed_mapping[tes_img]
  hp="<start> "+ans.strip()+" <end>"
  #print(hp)
  #print(lis_cap)
  #print(ans)
  bleu_score_cal(lis_cap,hp)
  gleu_score_cal(lis_cap,hp)
  roug_score_cal(lis_cap,hp)

def beam_search_pred(tes_img, K_beams = 3, log = False):
    start = [wd_ind["<start>"]]
    
    start_word = [[start, 0.0]]
    
    while len(start_word[0][0]) < max_len:
        temp = []
        for s in start_word:
            sequence  = pad_sequences([s[0]], maxlen=max_len).reshape((1,max_len)) #sequence of most probable words 
            i1=features[tes_img][0]                                                                    # based on the previous steps
            preds = model.predict([i1.reshape(1,2048), sequence])
            word_preds = np.argsort(preds[0])[-K_beams:] # sort predictions based on the probability, then take the last
                                                         # K_beams items. words with the most probs
            # Getting the top <K_beams>(n) predictions and creating a 
            # new list so as to put them via the model again
            for w in word_preds:
                
                next_cap, prob = s[0][:], s[1]
                next_cap.append(w)
                if log:
                    prob += np.log(preds[0][w]) # assign a probability to each K words4
                else:
                    prob += preds[0][w]
                temp.append([next_cap, prob])
        start_word = temp
        # Sorting according to the probabilities
        start_word = sorted(start_word, reverse=False, key=lambda l: l[1])

        # Getting the top words
        start_word = start_word[-K_beams:]
    
    start_word = start_word[-1][0]
    captions_ = [ind_wd[i] for i in start_word]
    #print(captions_)
    final_caption = []
    
    for i in captions_:
        if i != "<end>":
            final_caption.append(i)
        else:
            break
    
    final_caption = ' '.join(final_caption[1:])
    return final_caption
print("Beam 5")
a1=0;b1=0;c1=0;d1=0;e1=0;f1=0;g1=0;h1=0;i11=0
lnt=0
for i in range(0,len(test)):
  lnt+=1
  beam_3=beam_search_pred(test[i],  K_beams = 5, log = False)
  if lnt<=5:
    x=plt.imread("/content/drive/My Drive/ML_PROJECT_FOLDER/Flicker8k_Dataset/"+test[i])
    plt.imshow(x)
    plt.show()
    print(beam_3)
  st=beam_3.split(" ")
  ans=""
  for i in range(1,len(st)-1):
    ans+=st[i]+" "
  lis_cap=processed_mapping[tes_img]
  hp="<start> "+ans.strip()+" <end>"
  #print(hp)
  #print(lis_cap)
  #print(ans)
  t1,t2,t3,t4,t5=bleu_score_cal(lis_cap,hp)
  a1+=t1;b1+=t2;c1+=t3;d1+=t4;e1+=t5
  f1+=gleu_score_cal(lis_cap,hp)
  t1,t2,t3=roug_score_cal(lis_cap,hp)
  g1+=t1;h1+=t2;i11+=t3
  
print("N-grams: 1: {}, 2: {}, 3: {}, 4: {}, n: {} ".format(a1/len(test), b1/len(test),c1/len(test),d1/len(test),e1/len(test)))
print("Gleu Score: {}".format(f1/len(test)))
print("Rouge Score: f: {}, P: {}, R: {}".format(g1/len(test),h1/len(test),i11/len(test)))

def beam_search_pred(tes_img, K_beams = 3, log = False):
    start = [wd_ind["<start>"]]
    
    start_word = [[start, 0.0]]
    
    while len(start_word[0][0]) < max_len:
        temp = []
        for s in start_word:
            sequence  = pad_sequences([s[0]], maxlen=max_len).reshape((1,max_len)) #sequence of most probable words 
            i1=features[tes_img][0]                                                                    # based on the previous steps
            preds = model.predict([i1.reshape(1,2048), sequence])
            word_preds = np.argsort(preds[0])[-K_beams:] # sort predictions based on the probability, then take the last
                                                         # K_beams items. words with the most probs
            # Getting the top <K_beams>(n) predictions and creating a 
            # new list so as to put them via the model again
            for w in word_preds:
                
                next_cap, prob = s[0][:], s[1]
                next_cap.append(w)
                if log:
                    prob += np.log(preds[0][w]) # assign a probability to each K words4
                else:
                    prob += preds[0][w]
                temp.append([next_cap, prob])
        start_word = temp
        # Sorting according to the probabilities
        start_word = sorted(start_word, reverse=False, key=lambda l: l[1])

        # Getting the top words
        start_word = start_word[-K_beams:]
    
    start_word = start_word[-1][0]
    captions_ = [ind_wd[i] for i in start_word]
    #print(captions_)
    final_caption = []
    
    for i in captions_:
        if i != "<end>":
            final_caption.append(i)
        else:
            break
    
    final_caption = ' '.join(final_caption[1:])
    return final_caption
print("Beam 7")
for i in range(0,5):
  beam_3=beam_search_pred(test[i],  K_beams = 7, log = False)
  x=plt.imread("/content/drive/My Drive/ML_PROJECT_FOLDER/Flicker8k_Dataset/"+test[i])
  plt.imshow(x)
  plt.show()
  print(beam_3)
  st=beam_3.split(" ")
  ans=""
  for i in range(1,len(st)-1):
    ans+=st[i]+" "
  lis_cap=processed_mapping[tes_img]
  hp="<start> "+ans.strip()+" <end>"
  #print(hp)
  #print(lis_cap)
  #print(ans)
  bleu_score_cal(lis_cap,hp)
  gleu_score_cal(lis_cap,hp)
  roug_score_cal(lis_cap,hp)