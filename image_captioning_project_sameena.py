# -*- coding: utf-8 -*-
"""image_captioning_project_sameena.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BwTFSpwqJ9Y3LjYGQ9dVxQv0PgE631IW
"""

!pip install rouge

import numpy as np
import pandas as pd
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
import string
import pickle
from keras.applications.xception import Xception, preprocess_input
import os
import glob 
import matplotlib.pyplot as plt 

from PIL import Image
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from keras.layers.merge import add
from keras.models import Model, load_model
from keras.layers import Input, Dense, LSTM, Embedding, Dropout
from keras.utils import plot_model
import nltk.translate.bleu_score as bleu
import nltk.translate.gleu_score as gleu
from rouge import Rouge

from google.colab import drive
drive.mount('/content/drive')

from numpy import array

import nltk
nltk.download('punkt')
nltk.download('stopwords')

processed_mapping={}
vocab=[]

def processing_tokens(text):
  tokens=word_tokenize(text)
  table=str.maketrans('','',string.punctuation)
  ptokens=[w.translate(table) for w in tokens]
  stri=[]
  for wd in ptokens:
    if wd.isalpha() and len(wd)>1:
      stri.append(wd)
  #stop_words = set(stopwords.words('english'))
  #words = [t for t in stri if not t in stop_words]
  normal_tokens=[w.lower() for w in stri]
 
  non_blank_tokens=[s for s in normal_tokens if s!='']

  processed_string=""
  for st in non_blank_tokens:
    processed_string+=" "+st
  processed_string=processed_string.strip()
  new_str=""
  new_str="<start> "+processed_string+" <end>"
  return new_str

def make_dictinary():
  f1=open("/content/drive/My Drive/ML_PROJECT_FOLDER/Flickr8k.token.txt","r")
  token_read=f1.read()
  tk=token_read.split("\n")
  #print(tk[-1])\
  tk=tk[:-1]
  dic_mapping={}
  for value in tk:
    map=value.split("\t")
    #print(map[1])
    tokens=processing_tokens(map[1])

    name,tag=map[0].split("#")
    try:
      dic_mapping[name].append(tokens)
    
    except:
      dic_mapping[name]=[]
      dic_mapping[name].append(tokens)
  return dic_mapping

def data_preprocessing():
  global processed_mapping;global vocab
  processed_mapping=make_dictinary()
  t=set()
  for key in processed_mapping.keys():
    for i in range(0,len(processed_mapping[key])):
      ll=processed_mapping[key][i].split(' ')
      for val in ll:
        if val!='' and val!=' ':
          t.add(val)
  tt=list(t)
  #print(tt)
  vocab.extend(tt)
  #print(processed_mapping)
  print(vocab[:5])
data_preprocessing()

def load_data(d,name):
    dbfile = open(d, 'wb') 
    pickle.dump(name, dbfile)
    dbfile.close()
def find_data(name):
    a= open(name, 'rb')
    p= pickle._Unpickler(a)
    p.encoding='latin1'
    db=p.load()
    return db
#load_data("processed_map",processed_mapping)
#global processed_mapping
#processed_mapping=find_data(processed_mapping)

img_feature={}
def feature_extraction():
  global img_feature
  path="/content/drive/My Drive/ML_PROJECT_FOLDER/Flicker8k_Dataset/"
  model = Xception( include_top=False, pooling='avg' )
  for name in glob.glob(path+'*'):
    ig = Image.open(name)
    ig = ig.resize((299,299))
    ig= np.expand_dims(ig, axis=0)
    ig = ig/127.5;ig = ig - 1.0
    cal=model.predict(ig)
    sp=name.split('/')
    img_feature[sp[-1]]=cal
#feature_extraction()

#The feature extracted from the image has a size of 2048, 
#with a dense layer, we will reduce the dimensions to 256 nodes.

features = find_data("/content/drive/My Drive/ML_PROJECT_FOLDER/features.p")

wd_ind={}
ind_wd={}
for i in range(0,len(vocab)):
  wd_ind[vocab[i]]=i
  ind_wd[i]=vocab[i]

train=[]
test=[]
f1=open("/content/drive/My Drive/ML_PROJECT_FOLDER/train_img.txt","r")
tr=f1.read()
train=tr.split("\n")
f1=open("/content/drive/My Drive/ML_PROJECT_FOLDER/test_img.txt","r")
te=f1.read()
test=te.split("\n")

print((train))
print(test)
print(features["2513260012_03d33305cf.jpg"])
print(type(features))

max_len=0
def maximum_description():
  global processed_mapping; global max_len;tt=0
  for key in processed_mapping.keys():
    for cap in processed_mapping[key]:
      cp=cap.split(" ")
      #print(cp)
      max_len=max(max_len,len(cp))
  print(max_len,tt)
maximum_description()

from keras.preprocessing.text import Tokenizer
lis_data=[]
for key in processed_mapping.keys():
  for value in processed_mapping[key]:
    lis_data.append(value)
tokenizer = Tokenizer()
tokenizer.fit_on_texts(lis_data)

print(processed_mapping)

print(len(tokenizer.word_index))
print(len(vocab))
#vocab_size = len(tokenizer.word_index) + 1

print(tokenizer.texts_to_sequences("<start>"))

def batch_generation(num_step):
  img_inp=[];text_inp=[];out_pred=[];
  global max_len;global processed_mapping;global features;
  lim=0
  while 1:
    for key in train:
      lim+=1
      ft=features[key][0]
      for value in processed_mapping[key]:
        embed=tokenizer.texts_to_sequences([value])[0]
        #embed=seq = [wd_ind[word] for word in value.split(' ') if word in wd_ind]

        for ind in range(0,len(embed)):
          a=embed[:ind]
          b=embed[ind]
          a=pad_sequences([a], maxlen=max_len)[0]
          b = to_categorical([b], num_classes=len(tokenizer.word_index) + 1)[0]
          img_inp.append(ft);text_inp.append(a);out_pred.append(b)
      if lim==num_step:
        yield ([array(img_inp), array(text_inp)], array(out_pred))
        img_inp=[];text_inp=[];out_pred=[];
        lim=0
#[s,t],u=next(batch_generation(3))
#print(s.shape,t.shape,u.shape)

'''inputs1 = Input(shape=(2048,))
a1 = Dropout(0.5)(inputs1)
a2 = Dense(256, activation='relu')(a1)
inputs2 = Input(shape=(max_len,))
b1 = Embedding(len(tokenizer.word_index) + 1, 256, mask_zero=True)(inputs2)
b2 = Dropout(0.5)(b1)
b3 = LSTM(256)(b2)
decoder1 = add([a2, b3])
decoder2 = Dense(256, activation='relu')(decoder1)
outputs = Dense(len(tokenizer.word_index) + 1, activation='softmax')(decoder2)
model = Model(inputs=[inputs1, inputs2], outputs=outputs)
model.compile(loss='categorical_crossentropy', optimizer='adam')    
print(model.summary())'''


inputs1 = Input(shape=(2048,))
fe1 = Dropout(0.5)(inputs1)
fe2 = Dense(256, activation='relu')(fe1)
# LSTM sequence model
inputs2 = Input(shape=(max_len,))
se1 = Embedding(len(tokenizer.word_index) + 1, 256, mask_zero=True)(inputs2)
se2 = Dropout(0.5)(se1)
se3 = LSTM(256)(se2)
# Merging both models
decoder1 = add([fe2, se3])
decoder2 = Dense(256, activation='relu')(decoder1)
outputs = Dense(len(tokenizer.word_index) + 1, activation='softmax')(decoder2)    
# tie it together [image, seq] [word]
model = Model(inputs=[inputs1, inputs2], outputs=outputs)
model.compile(loss='categorical_crossentropy', optimizer='adam')    
# summarize model
print(model.summary())
#plot_model(model, to_file='model.png', show_shapes=True)

epochs = 50
num_per_batch = 3
steps = len(train)//num_per_batch
from keras.callbacks import History 
H = History()
#model.layers[2].set_weights([embedding_matrix])
#model.layers[2].trainable = False
train_loss=[]
for i in range(epochs):
    generator = batch_generation(num_per_batch)
    H=model.fit_generator(generator,epochs=1, steps_per_epoch=steps, verbose=1)
    train_loss.append(H.history["loss"])
    #model.save('model_' + str(i) + '.h5')

#model.save('/content/drive/My Drive/model_final')

epochs = 10
num_per_batch = 3
steps = len(train)//num_per_batch
from keras.callbacks import History 
H = History()
#model.layers[2].set_weights([embedding_matrix])
#model.layers[2].trainable = False
train_loss=[]
for i in range(epochs):
    generator = batch_generation(num_per_batch)
    H=model.fit_generator(generator,epochs=1, steps_per_epoch=steps, verbose=1)
    #model.save('model_' + str(i) + '.h5')

epochs = 30
num_per_batch = 3
steps = len(train)//num_per_batch
from keras.callbacks import History 
H = History()
#model.layers[2].set_weights([embedding_matrix])
#model.layers[2].trainable = False
train_loss=[]
for i in range(epochs):
    generator = batch_generation(num_per_batch)
    H=model.fit_generator(generator,epochs=1, steps_per_epoch=steps, verbose=1)
    #model.save('model_' + str(i) + '.h5')

#model.save('/content/drive/My Drive/model_final_90_epoch')

import keras
model = keras.models.load_model('/content/drive/My Drive/model_final_90_epoch')

x_ax=[1,5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90]
y_ax=[3.3957,2.6189,2.2642,2.0960,1.9908,1.9196,1.8714,1.8410,1.8111,1.7884,1.7709,1.7666,1.7505,
      1.7422,1.7327,1.7261,1.7215,1.7171,1.7200]
plt.plot(x_ax,y_ax) 
plt.xlabel('Epoch') 
plt.ylabel('loss') 
plt.title('Training loss epoch curve')  
plt.show()

def bleu_score_cal(reference,hypothesis):
  ref=[];hyp=hypothesis.split(" ")
  for value in reference:
    t=value.split(" ")
    ref.append(t)
  #print("BLEU SCORE")
  score_1gram = bleu.sentence_bleu(ref, hyp, weights=(1,0,0,0))
  score_2gram = bleu.sentence_bleu(ref, hyp, weights=(0,1,0,0))
  score_3gram = bleu.sentence_bleu(ref, hyp, weights=(0,0,1,0))
  score_4gram = bleu.sentence_bleu(ref, hyp, weights=(0,0,0,1))
  #print("N-grams: 1: {}, 2: {}, 3: {}, 4: {}".format(score_1gram, score_2gram, score_3gram, score_4gram))
  score_ngram = bleu.sentence_bleu(ref, hyp, weights=(0.25,0.25,0.25,0.25))
  #print("N-grams: {} ".format( score_ngram))
  return score_1gram,score_2gram,score_3gram,score_4gram,score_ngram,

def gleu_score_cal(reference,hypothesis):
  ref=[];hyp=hypothesis.split(" ")
  for value in reference:
    t=value.split(" ")
    ref.append(t)
  score_ref_ab = gleu.sentence_gleu(ref, hyp)
  #print("Gleu Score: {}".format(score_ref_ab))
  return score_ref_ab

def roug_score_cal(reference,hypothesis):
  rouge = Rouge()
  f_s=0;p_s=0;r_s=0
  
  for value in reference:
    scores = rouge.get_scores(hypothesis,value)
    f_s+=float(scores[0]["rouge-l"]["f"])
    p_s+=float(scores[0]["rouge-l"]["p"])
    r_s+=float(scores[0]["rouge-l"]["r"])
  #print("Rouge Score: f: {}, P: {}, R: {}".format(f_s/5,p_s/5,r_s/5))
  return f_s/5,p_s/5,r_s/5

test=test[:-1]

import matplotlib.pyplot as plt
ll=0;
a1=0;b1=0;c1=0;d1=0;e1=0;f1=0;g1=0;h1=0;i11=0

for tes_img in test:
  ll+=1
  generated_text="<start>"
  for i in range(0,max_len):
    emb=tokenizer.texts_to_sequences([generated_text])[0]
    emb=pad_sequences([emb], maxlen=max_len)[0]
    i1=features[tes_img][0].reshape(1,-1)
    i2=emb.reshape(1,-1)
    next_word= model.predict_on_batch([i1,i2])
    pred_arg = np.argmax(next_word)
    value=None
    for a, b in tokenizer.word_index.items():
      if b == pred_arg:
        value=a
        break
    if value!=None:
      generated_text+=" "+value
    if value=="end":
      break
  #print("For image")
  if ll<=5:
    x=plt.imread("/content/drive/My Drive/ML_PROJECT_FOLDER/Flicker8k_Dataset/"+tes_img)
    plt.imshow(x)
    plt.show()
    st=generated_text.split(" ")
    ans=""
    for i in range(1,len(st)-1):
      ans+=st[i]+" "
    print(ans)
  lis_cap=processed_mapping[tes_img]
  hp="<start> "+ans.strip()+" <end>"
  #print(hp)
  #print(lis_cap)
  t1,t2,t3,t4,t5=bleu_score_cal(lis_cap,hp)
  a1+=t1;b1+=t2;c1+=t3;d1+=t4;e1+=t5
  f1+=gleu_score_cal(lis_cap,hp)
  t1,t2,t3=roug_score_cal(lis_cap,hp)
  g1+=t1;h1+=t2;i11+=t3
  
print("N-grams: 1: {}, 2: {}, 3: {}, 4: {}, n: {} ".format(a1/len(test), b1/len(test),c1/len(test),d1/len(test),e1/len(test)))
print("Gleu Score: {}".format(f1/len(test)))
print("Rouge Score: f: {}, P: {}, R: {}".format(g1/len(test),h1/len(test),i11/len(test)))

"""BEAM SEARCH"""

def beam_search_pred(tes_img, K_beams = 3, log = False):
    start = [wd_ind["<start>"]]
    start_word = [[start, 0.0]]
    while len(start_word[0][0]) < max_len:
        temp = []
        for s in start_word:
            sequence  = pad_sequences([s[0]], maxlen=max_len).reshape((1,max_len)) #sequence of most probable words 

            i1=features[tes_img][0]                                                                    # based on the previous steps
            preds = model.predict([i1.reshape(1,2048), sequence])
            word_preds = np.argsort(preds[0])[-K_beams:] # sort predictions based on the probability, then take the last
                                                         # K_beams items. words with the most probs
            # Getting the top <K_beams>(n) predictions and creating a 
            # new list so as to put them via the model again
            for w in word_preds:
                
                next_cap, prob = s[0][:], s[1]
                next_cap.append(w)
                if log:
                    prob += np.log(preds[0][w]) # assign a probability to each K words4
                else:
                    prob += preds[0][w]
                temp.append([next_cap, prob])
        start_word = temp
        # Sorting according to the probabilities
        start_word = sorted(start_word, reverse=False, key=lambda l: l[1])

        # Getting the top words
        start_word = start_word[-K_beams:]
    
    start_word = start_word[-1][0]
    captions_=[]
    for i in start_word:
      for a, b in tokenizer.word_index.items():
        if b==i:
          captions_.append(a)
          break
      


    # = [ind_wd[i] for i in start_word]
    #print(captions_)
    final_caption = []
    
    for i in captions_:
        if i != "<end>":
            final_caption.append(i)
        else:
            break
    
    final_caption = ' '.join(final_caption[1:])
    return final_caption
print("Beam 3")
for i in range(0,5):
  beam_3=beam_search_pred(test[i],  K_beams = 3, log = False)
  x=plt.imread("/content/drive/My Drive/ML_PROJECT_FOLDER/Flicker8k_Dataset/"+test[i])
  plt.imshow(x)
  plt.show()
  print(beam_3)
  st=beam_3.split(" ")
  ans=""
  for i in range(1,len(st)-1):
    ans+=st[i]+" "
  lis_cap=processed_mapping[tes_img]
  hp="<start> "+ans.strip()+" <end>"
  #print(hp)
  #print(lis_cap)
  #print(ans)
  bleu_score_cal(lis_cap,hp)
  gleu_score_cal(lis_cap,hp)
  roug_score_cal(lis_cap,hp)

def beam_search_pred(tes_img, K_beams = 3, log = False):
    start = [wd_ind["<start>"]]
    start_word = [[start, 0.0]]
    while len(start_word[0][0]) < max_len:
        temp = []
        for s in start_word:
            sequence  = pad_sequences([s[0]], maxlen=max_len).reshape((1,max_len)) #sequence of most probable words 

            i1=features[tes_img][0]                                                                    # based on the previous steps
            preds = model.predict([i1.reshape(1,2048), sequence])
            word_preds = np.argsort(preds[0])[-K_beams:] # sort predictions based on the probability, then take the last
                                                         # K_beams items. words with the most probs
            # Getting the top <K_beams>(n) predictions and creating a 
            # new list so as to put them via the model again
            for w in word_preds:
                
                next_cap, prob = s[0][:], s[1]
                next_cap.append(w)
                if log:
                    prob += np.log(preds[0][w]) # assign a probability to each K words4
                else:
                    prob += preds[0][w]
                temp.append([next_cap, prob])
        start_word = temp
        # Sorting according to the probabilities
        start_word = sorted(start_word, reverse=False, key=lambda l: l[1])

        # Getting the top words
        start_word = start_word[-K_beams:]
    
    start_word = start_word[-1][0]
    captions_=[]
    for i in start_word:
      for a, b in tokenizer.word_index.items():
        if b==i:
          captions_.append(a)
          break
      


    # = [ind_wd[i] for i in start_word]
    #print(captions_)
    final_caption = []
    
    for i in captions_:
        if i != "<end>":
            final_caption.append(i)
        else:
            break
    
    final_caption = ' '.join(final_caption[1:])
    return final_caption

print("Beam 5")
a1=0;b1=0;c1=0;d1=0;e1=0;f1=0;g1=0;h1=0;i11=0
lnt=0
for i in range(0,len(test)):
  lnt+=1
  
  beam_3=beam_search_pred(test[i],  K_beams = 5, log = False)
  if lnt<=5:
    x=plt.imread("/content/drive/My Drive/ML_PROJECT_FOLDER/Flicker8k_Dataset/"+test[i])
    plt.imshow(x)
    plt.show()
    print(beam_3)
  st=beam_3.split(" ")
  ans=""
  for i in range(1,len(st)-1):
    ans+=st[i]+" "
  lis_cap=processed_mapping[tes_img]
  hp="<start> "+ans.strip()+" <end>"
  #print(hp)
  #print(lis_cap)
  #print(ans)
  t1,t2,t3,t4,t5=bleu_score_cal(lis_cap,hp)
  a1+=t1;b1+=t2;c1+=t3;d1+=t4;e1+=t5
  f1+=gleu_score_cal(lis_cap,hp)
  t1,t2,t3=roug_score_cal(lis_cap,hp)
  g1+=t1;h1+=t2;i11+=t3
  
print("N-grams: 1: {}, 2: {}, 3: {}, 4: {}, n: {} ".format(a1/len(test), b1/len(test),c1/len(test),d1/len(test),e1/len(test)))
print("Gleu Score: {}".format(f1/len(test)))
print("Rouge Score: f: {}, P: {}, R: {}".format(g1/len(test),h1/len(test),i11/len(test)))

def beam_search_pred(tes_img, K_beams = 3, log = False):
    start = [wd_ind["<start>"]]
    start_word = [[start, 0.0]]
    while len(start_word[0][0]) < max_len:
        temp = []
        for s in start_word:
            sequence  = pad_sequences([s[0]], maxlen=max_len).reshape((1,max_len)) #sequence of most probable words 

            i1=features[tes_img][0]                                                                    # based on the previous steps
            preds = model.predict([i1.reshape(1,2048), sequence])
            word_preds = np.argsort(preds[0])[-K_beams:] # sort predictions based on the probability, then take the last
                                                         # K_beams items. words with the most probs
            # Getting the top <K_beams>(n) predictions and creating a 
            # new list so as to put them via the model again
            for w in word_preds:
                
                next_cap, prob = s[0][:], s[1]
                next_cap.append(w)
                if log:
                    prob += np.log(preds[0][w]) # assign a probability to each K words4
                else:
                    prob += preds[0][w]
                temp.append([next_cap, prob])
        start_word = temp
        # Sorting according to the probabilities
        start_word = sorted(start_word, reverse=False, key=lambda l: l[1])

        # Getting the top words
        start_word = start_word[-K_beams:]
    
    start_word = start_word[-1][0]
    captions_=[]
    for i in start_word:
      for a, b in tokenizer.word_index.items():
        if b==i:
          captions_.append(a)
          break
      


    # = [ind_wd[i] for i in start_word]
    #print(captions_)
    final_caption = []
    
    for i in captions_:
        if i != "<end>":
            final_caption.append(i)
        else:
            break
    
    final_caption = ' '.join(final_caption[1:])
    return final_caption
print("Beam 7")
for i in range(0,5):
  beam_3=beam_search_pred(test[i],  K_beams = 7, log = False)
  x=plt.imread("/content/drive/My Drive/ML_PROJECT_FOLDER/Flicker8k_Dataset/"+test[i])
  plt.imshow(x)
  plt.show()
  print(beam_3)
  st=beam_3.split(" ")
  ans=""
  for i in range(1,len(st)-1):
    ans+=st[i]+" "
  lis_cap=processed_mapping[tes_img]
  hp="<start> "+ans.strip()+" <end>"
  #print(hp)
  #print(lis_cap)
  #print(ans)
  bleu_score_cal(lis_cap,hp)
  gleu_score_cal(lis_cap,hp)
  roug_score_cal(lis_cap,hp)



#https://github.com/yashk2810/Image-Captioning (beam serarch this we do in future)







loss=[4.2564,3.4159,3.1144,2.9105,2.7547,2.6277,2.5245,2.4397,2.3722,2.3137,2.2663,2.2224,
      2.1838,2.1526,2.1248,2.0924,2.0696,2.0514,2.0297,2.0084]
epoch=range(1,61)
plt.plot(range(1,epochs+1), train_loss) 
plt.xlabel('Epoch') 
plt.ylabel('loss') 
plt.title('Training loss epoch curve')  
plt.show()

"""See only last epoch loss is stored. Use this syntax to store loss at all epochs like **loss_list.extend(history.history['loss'])**"""

history.history['loss']

'''GLEU: Google-BLEU
NLP evaluation metric used in Machine Translation tasks

Suitable for measuring sentence level similarity

Range: 0 (no match) to 1 (exact match)'''
import nltk.translate.gleu_score as gleu
hyp = str('she read the book because she was interested in world history').split()
ref_a = str('she read the book because she was interested in world history').split()
ref_b = str('she was interested in world history because she read the book').split()
score_ref_a = gleu.sentence_gleu([ref_a], hyp)
print("Hyp and ref_a are the same: {}".format(score_ref_a))
score_ref_b = gleu.sentence_gleu([ref_b], hyp)
print("Hyp and ref_b are different: {}".format(score_ref_b))
score_ref_ab = gleu.sentence_gleu([ref_a, ref_b], hyp)
print("Hyp vs multiple refs: {}".format(score_ref_ab))

'''BLEU: BiLingual Evaluation Understudy
NLP evaluation metric used in Machine Translation tasks

Suitable for measuring corpus level similarity

$n$-gram comparison between words in candidate sentence and reference sentences

Range: 0 (no match) to 1 (exact match)'''
import nltk.translate.bleu_score as bleu
score_ref_a = bleu.sentence_bleu([ref_a], hyp)
print("Hyp and ref_a are the same: {}".format(score_ref_a))
score_ref_b = bleu.sentence_bleu([ref_b], hyp)
print("Hyp and ref_b are different: {}".format(score_ref_b))
score_ref_ab = bleu.sentence_bleu([ref_a, ref_b], hyp)
print("Hyp vs multiple refs: {}".format(score_ref_ab))

score_1gram = bleu.sentence_bleu([ref_b], hyp, weights=(1,0,0,0))
score_2gram = bleu.sentence_bleu([ref_b], hyp, weights=(0,1,0,0))
score_3gram = bleu.sentence_bleu([ref_b], hyp, weights=(0,0,1,0))
score_4gram = bleu.sentence_bleu([ref_b], hyp, weights=(0,0,0,1))
print("N-grams: 1-{}, 2-{}, 3-{}, 4-{}".format(score_1gram, score_2gram, score_3gram, score_4gram))

#Cumulative N-grams: by default, the score is calculatedby considering all $N$-grams equally in a geometric mea
score_ngram1 = bleu.sentence_bleu([ref_b], hyp)
score_ngram = bleu.sentence_bleu([ref_b], hyp, weights=(0.25,0.25,0.25,0.25))
score_ngram_geo = (11/11*9/10*6/9*4/8)**0.25
print("N-grams: {} = {} = ".format(score_ngram1, score_ngram, score_ngram_geo))

from rouge import Rouge 
#!pip install rouge
rouge = Rouge()
hypothesis="sameena is a"
reference="sameena firdos"
scores = rouge.get_scores(hypothesis, reference)
print(scores)
#https://kavita-ganesan.com/what-is-rouge-and-how-it-works-for-evaluation-of-summaries/#.X5ch6IgzY2w





#  https://colab.research.google.com/gist/amahendrakar/39db0b14bce096a12d6f4c9961f687de/42038.ipynb#scrollTo=BCcpB-9_Pni4









print(tokenizer.texts_to_sequences(["girl is playing on hill"]))



https://github.com/hlamba28/Automatic-Image-Captioning/blob/master/Automatic%20Image%20Captioning.ipynb
(see above)

def data():
  while 1:
    for i in range (0,10):
      m=i+i
      yield [i]
for l in range(0,10):
  val=next(data())
  print(val)

import matplotlib.pyplot as plt
N = np.arange(0, 50)
plt.style.use("ggplot")
plt.figure(figsize=(16,9))
plt.title("Training Loss and Accuracy (Dense NN)")
plt.plot(N, H.history["loss"], label="train_loss")
plt.plot(N, H.history["val_loss"], label="test_loss")
plt.plot(N, H.history["accuracy"], label="train_acc")
plt.plot(N, H.history["val_accuracy"], label="test_acc")
#plt.plot(N, H.history["aucroc"], label="train_rocauc")
#plt.plot(N, H.history["val_aucroc"], label="test_rocauc")
#plt.scatter([11],[0.789])
plt.xlabel("Epoch #")
plt.ylabel("Loss/Accuracy")
plt.legend(loc="best")
plt.show()



